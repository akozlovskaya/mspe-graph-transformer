# Training configuration for LRGB benchmarks
epochs: 200
batch_size: 64
num_workers: 4
grad_clip: 1.0
mixed_precision: false
log_every: 50
metric_for_best: loss
minimize_metric: true
early_stopping: 50

optimizer:
  type: adamw
  lr: 3e-4
  weight_decay: 0.01
  betas: [0.9, 0.999]
  eps: 1e-8

scheduler:
  type: cosine_warmup
  warmup_epochs: 20
  min_lr: 1e-6

