# Graph Transformer Models

ĞœĞ¾Ğ´ÑƒĞ»ÑŒ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·ÑƒÑÑ‰Ğ¸Ğ¹ Graph Transformer Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ² ÑÑ‚Ğ¸Ğ»Ğµ GraphGPS Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹ multi-scale positional encodings.

## ğŸ“¦ ĞÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹

### 1. **GraphTransformer** - ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ

```python
from src.models import GraphTransformer

model = GraphTransformer(
    node_dim=9,            # Ğ Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒ node features
    hidden_dim=128,        # Ğ¡ĞºÑ€Ñ‹Ñ‚Ğ¾Ğµ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğµ
    num_layers=12,         # ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ GPS ÑĞ»Ğ¾Ñ‘Ğ²
    num_heads=8,           # ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ attention heads
    out_dim=1,             # Ğ’Ñ‹Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğµ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ğµ
    mpnn_type="gin",       # Ğ¢Ğ¸Ğ¿ MPNN: 'gin', 'gat', 'gcn'
    node_pe_dim=16,        # Ğ Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒ node PE (0 = Ğ±ĞµĞ· PE)
    use_relative_pe=True,  # Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ relative PE
    dropout=0.1,
    task="graph",          # 'graph' Ğ¸Ğ»Ğ¸ 'node'
)

# Forward pass
out = model(data)  # data.x, data.edge_index, data.node_pe, data.edge_pe
```

### 2. **GPSLayer** - ĞÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ±Ğ»Ğ¾Ğº

```python
from src.models import GPSLayer

layer = GPSLayer(
    hidden_dim=128,
    num_heads=8,
    mpnn_type="gin",
    dropout=0.1,
    gate_type="scalar",  # 'scalar', 'vector', 'mlp'
    use_local=True,      # Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ local MPNN
    use_global=True,     # Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ global attention
)

out = layer(x, edge_index, attention_bias=bias)
```

### 3. **MultiHeadAttention** - Attention Ñ relative PE

```python
from src.models import MultiHeadAttention

attn = MultiHeadAttention(
    hidden_dim=128,
    num_heads=8,
    dropout=0.1,
)

# Ğ¡ attention bias Ğ¾Ñ‚ relative PE
out = attn(x, attention_bias=bias, batch=batch)
```

### 4. **MPNNBlock** - Local message passing

```python
from src.models import MPNNBlock

# GIN
mpnn = MPNNBlock(hidden_dim=128, mpnn_type="gin")

# GAT
mpnn = MPNNBlock(hidden_dim=128, mpnn_type="gat", num_heads=4)

# GCN
mpnn = MPNNBlock(hidden_dim=128, mpnn_type="gcn")

out = mpnn(x, edge_index)
```

## ğŸ—ï¸ ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°

```
Input: data.x [N, node_dim], data.node_pe [N, pe_dim], data.edge_pe [P, pe_dim]
                                    â”‚
                                    â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚     Node PE Integration       â”‚
                    â”‚   [x, pe] â†’ Linear â†’ hidden   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚        GPS Layer Ã— L          â”‚
                    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
                    â”‚ â”‚ Local     â”‚ â”‚ Global    â”‚   â”‚
                    â”‚ â”‚ MPNN      â”‚ â”‚ Attention â”‚   â”‚
                    â”‚ â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜   â”‚
                    â”‚       â”‚   Gate Mixing â”‚       â”‚
                    â”‚       â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
                    â”‚              â”‚                â”‚
                    â”‚       â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”        â”‚
                    â”‚       â”‚     FFN     â”‚        â”‚
                    â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚     Readout + Pred Head       â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
                              Output [B, out_dim]
```

## ğŸ“Š Ğ’Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ

ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¾Ğ¶Ğ¸Ğ´Ğ°ĞµÑ‚ PyG Data Ğ¸Ğ»Ğ¸ Batch Ñ Ğ¿Ğ¾Ğ»ÑĞ¼Ğ¸:

- `x`: Node features [N, node_dim]
- `edge_index`: Edge indices [2, E]
- `node_pe`: Node positional encodings [N, pe_dim] (Ğ¾Ğ¿Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾)
- `edge_pe_index`: Relative PE indices [2, P] (Ğ¾Ğ¿Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾)
- `edge_pe`: Relative PE values [P, pe_dim] (Ğ¾Ğ¿Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾)
- `batch`: Batch assignment [N] (Ğ´Ğ»Ñ Batch)

## ğŸ”§ ĞÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸

### Pre-LN Normalization
Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Pre-LN (LayerNorm Ğ¿ĞµÑ€ĞµĞ´ ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¼ sub-block) Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.

### Gating Mechanism
Learnable gate Ğ´Ğ»Ñ ÑĞ¼ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ñ local (MPNN) Ğ¸ global (attention) features:
```
h = gate * h_global + (1 - gate) * h_local
```

### Stochastic Depth
Drop path Ğ´Ğ»Ñ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (linearly increasing rate).

### Relative PE Integration
Attention bias Ğ¸Ğ· relative PE:
```
attn = softmax(QK^T / âˆšd + bias)
```

## ğŸ§ª Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ

```bash
pytest tests/test_graph_transformer.py -v
```

## ğŸ“š ĞŸÑ€Ğ¸Ğ¼ĞµÑ€Ñ‹

### Graph Classification

```python
from src.models import GraphTransformer
from torch_geometric.data import DataLoader

model = GraphTransformer(
    node_dim=dataset.num_features,
    hidden_dim=128,
    num_layers=12,
    num_heads=8,
    out_dim=dataset.num_classes,
    mpnn_type="gin",
    node_pe_dim=16,
)

for batch in train_loader:
    out = model(batch)  # [B, num_classes]
    loss = criterion(out, batch.y)
```

### Node Classification

```python
model = GraphTransformer(
    node_dim=dataset.num_features,
    hidden_dim=128,
    num_layers=8,
    num_heads=8,
    out_dim=dataset.num_classes,
    task="node",
)

out = model(data)  # [N, num_classes]
```

### Getting Node Embeddings

```python
embeddings = model.get_node_embeddings(data)  # [N, hidden_dim]
```

## ğŸ“– Ğ¡ÑÑ‹Ğ»ĞºĞ¸

- [Recipe for a General, Powerful, Scalable Graph Transformer](https://arxiv.org/abs/2205.12454) (RampÃ¡Å¡ek et al., 2022)
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) (Vaswani et al., 2017)
- [Graph Attention Networks](https://arxiv.org/abs/1710.10903) (VeliÄkoviÄ‡ et al., 2018)

